{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac206671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.spatial.transform import Rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438611f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS = 1024\n",
    "OUTPUT_ROOT = \"Outputs\"\n",
    "MESH_DIR = \"meshes\"\n",
    "OUTPUT_ROOT_TASK4 = \"Outputs_Task4\"\n",
    "NUM_TRANSFORMATIONS = 5\n",
    "K_NEIGHBORS = 10\n",
    "ADAPTIVE_DENSITY_FACTOR = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze(filepath):\n",
    "    #Initial print\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"Analyzing: {os.path.basename(filepath)}\")\n",
    "    print('='*30)\n",
    "\n",
    "    # Mesh loading\n",
    "    mesh = trimesh.load(filepath, process=False)\n",
    "    vertices = np.array(mesh.vertices)  \n",
    "    faces = np.array(mesh.faces)\n",
    "        \n",
    "    num_vertices = vertices.shape[0]\n",
    "    num_faces = faces.shape[0]\n",
    "    \n",
    "    print(f\"Number of vertices: {num_vertices}\")\n",
    "    print(f\"Number of faces: {num_faces}\")\n",
    "    \n",
    "    #per-axis statistics\n",
    "    axis_names = np.array(['X', 'Y', 'Z'])\n",
    "    mins = np.min(vertices, axis=0)\n",
    "    maxs = np.max(vertices, axis=0)\n",
    "    means = np.mean(vertices, axis=0)\n",
    "    stds = np.std(vertices, axis=0)\n",
    "        \n",
    "    for i, axis in enumerate(axis_names):\n",
    "        print(f\"\\n{axis}-axis:\")\n",
    "        print(f\"  Min: {mins[i]:.4f}\")\n",
    "        print(f\"  Max: {maxs[i]:.4f}\")\n",
    "        print(f\"  Mean: {means[i]:.4f}\")\n",
    "        print(f\"  Std Dev: {stds[i]:.4f}\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(f\"\\nBounding box dimensions:\")\n",
    "    bbox_dims = maxs - mins\n",
    "    print(f\"Width (X): {bbox_dims[0]:.4f}\")\n",
    "    print(f\"Depth (Y): {bbox_dims[1]:.4f}\")\n",
    "    print(f\"Height (Z): {bbox_dims[2]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMesh centroid: ({means[0]:.4f}, {means[1]:.4f}, {means[2]:.4f})\")\n",
    "    \n",
    "    # Visualization                                \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2], triangles=faces, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "    \n",
    "    ax.set_title(f'{os.path.basename(filepath)}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mesh, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes = \"meshes/\"\n",
    "mesh_samples = np.array([i for i in os.listdir(meshes) if i.endswith('.obj')])\n",
    "\n",
    "print(f\"Found {len(mesh_samples)} sample mesh files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471547a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_filename in mesh_samples:\n",
    "    filepath = os.path.join(meshes, sample_filename)\n",
    "    mesh, vertices = load_and_analyze(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normalize(vertices):\n",
    "    \"\"\"\n",
    "    Min–Max Normalization: Scale vertex coordinates into [0, 1] per axis.\n",
    "    \"\"\"\n",
    "    v_min = vertices.min(axis=0)\n",
    "    v_max = vertices.max(axis=0)\n",
    "    # prevent divide-by-zero\n",
    "    range_ = v_max - v_min\n",
    "    if np.any(range_ == 0):\n",
    "        print(\"⚠️ Warning: Zero range axis detected; skipping normalization for that axis.\")\n",
    "        range_[range_ == 0] = 1.0\n",
    "    normalized = (vertices - v_min) / range_\n",
    "    return normalized, v_min, v_max\n",
    "\n",
    "def unit_sphere_normalize(vertices):\n",
    "    \"\"\"\n",
    "    Unit Sphere Normalization: Center vertices at origin and scale so that \n",
    "    the farthest vertex lies on a unit sphere (‖v‖ ≤ 1).\n",
    "    \"\"\"\n",
    "    centroid = vertices.mean(axis=0)\n",
    "    centered = vertices - centroid\n",
    "    scale = np.max(np.linalg.norm(centered, axis=1))\n",
    "    if scale == 0 or np.isclose(scale, 0): \n",
    "        scale = 1.0\n",
    "    normalized = centered / scale\n",
    "    \n",
    "    # Verify normalization\n",
    "    max_norm = np.max(np.linalg.norm(normalized, axis=1))\n",
    "    assert np.isclose(max_norm, 1.0, atol=1e-6), f\"Normalization failed: max_norm={max_norm}\"\n",
    "    \n",
    "    return normalized, centroid, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(normalized, bins=1024):\n",
    "    \"\"\"Discretize normalized coordinates into bins.\"\"\"\n",
    "    norm_copy = normalized.copy()\n",
    "    \n",
    "    # Determine if we need to shift based on range\n",
    "    data_min = norm_copy.min()\n",
    "    data_max = norm_copy.max()\n",
    "    \n",
    "    # If data spans negative values, it's in [-1, 1] range\n",
    "    shifted = (data_min < -0.01)  # Use small threshold for numerical stability\n",
    "    \n",
    "    if shifted:\n",
    "        # Shift from [-1, 1] to [0, 1]\n",
    "        norm_copy = (norm_copy + 1.0) / 2.0\n",
    "    \n",
    "    # Ensure we're in [0, 1]\n",
    "    norm_copy = np.clip(norm_copy, 0.0, 1.0)\n",
    "    \n",
    "    # Quantize to integer bins\n",
    "    quantized = np.floor(norm_copy * (bins - 1)).astype(np.int64)\n",
    "    quantized = np.clip(quantized, 0, bins - 1)  # Extra safety\n",
    "    \n",
    "    return quantized, shifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e205c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_task2(norm_mm, norm_us, quant_mm, quant_us, faces, sample_name, sample_dir):\n",
    "    \"\"\"\n",
    "    Generateing a 2×2 subplot for each mesh:\n",
    "    Row 1: Min–Max Normalized | Unit Sphere Normalized\n",
    "    Row 2: Min–Max Quantized  | Unit Sphere Quantized\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    import numpy as np, os\n",
    "\n",
    "    # #Downsample for speed\n",
    "    # def maybe_downsample(verts, max_verts=8000):\n",
    "    #     if len(verts) > max_verts:\n",
    "    #         idx = np.random.choice(len(verts), max_verts, replace=False)\n",
    "    #         return verts[idx]\n",
    "    #     return verts\n",
    "    \"\"\" Removed the downsampling function and passing the values as it is.\"\"\"\n",
    "    norm_mm_vis = norm_mm\n",
    "    norm_us_vis = norm_us\n",
    "    quant_mm_array, _ = quant_mm  # Extracting the array part\n",
    "    quant_us_array, _ = quant_us  # Extracting the array part\n",
    "\n",
    "    norm_mm_vis = norm_mm\n",
    "    norm_us_vis = norm_us\n",
    "    quant_mm_vis = quant_mm_array.astype(float)  # FIXED\n",
    "    quant_us_vis = quant_us_array.astype(float)\n",
    "\n",
    "    # --- Create figure ---\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 12), subplot_kw={'projection': '3d'})\n",
    "    fig.suptitle(f'Task 2 Results: {sample_name}', fontsize=15, fontweight='bold')\n",
    "\n",
    "    \n",
    "    def set_equal_axes(ax, data):\n",
    "        mins, maxs = data.min(axis=0), data.max(axis=0)\n",
    "        rng = maxs - mins\n",
    "        mid = (maxs + mins) / 2\n",
    "        max_range = rng.max() / 2\n",
    "        ax.set_xlim(mid[0]-max_range, mid[0]+max_range)\n",
    "        ax.set_ylim(mid[1]-max_range, mid[1]+max_range)\n",
    "        ax.set_zlim(mid[2]-max_range, mid[2]+max_range)\n",
    "\n",
    "  \n",
    "    plots = [\n",
    "        (axs[0,0], norm_mm_vis, 'Min–Max Normalized', 'skyblue'),\n",
    "        (axs[0,1], norm_us_vis, 'Unit Sphere Normalized', 'lightgreen'),\n",
    "        (axs[1,0], quant_mm_vis, 'Min–Max Quantized', 'orange'),\n",
    "        (axs[1,1], quant_us_vis, 'Unit Sphere Quantized', 'pink')\n",
    "    ]\n",
    "\n",
    "    for ax, data, title, color in plots:\n",
    "        ax.scatter(data[:,0], data[:,1], data[:,2], s=5, c=color, alpha=0.7)\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "        set_equal_axes(ax, data)\n",
    "        ax.grid(False)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    " \n",
    "    plot_path = os.path.join(sample_dir, \"plots\", \"task2_results.png\")\n",
    "    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "    plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Visualization saved {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca093322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_task2(mesh_path, sample_name):\n",
    "    \"\"\"\n",
    "    Process a single mesh for Task 2:\n",
    "    1. Load mesh\n",
    "    2. Apply both normalization methods\n",
    "    3. Quantize both\n",
    "    4. Save all four meshes (.ply)\n",
    "    5. Generate visualization\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {sample_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load mesh\n",
    "    mesh = trimesh.load(mesh_path, process=False)\n",
    "    vertices = np.array(mesh.vertices)\n",
    "    faces = np.array(mesh.faces)\n",
    "    \n",
    "    print(f\"Vertices: {len(vertices)}, Faces: {len(faces)}\")\n",
    "    \n",
    "    # Apply both normalization methods\n",
    "    print(\"\\n[1] Normalizing...\")\n",
    "    norm_mm, vmin, vmax = minmax_normalize(vertices)\n",
    "    norm_us, centroid, scale = unit_sphere_normalize(vertices)\n",
    "    print(f\" Min–Max range: [{norm_mm.min():.3f}, {norm_mm.max():.3f}]\")\n",
    "    print(f\" Unit Sphere range: [{norm_us.min():.3f}, {norm_us.max():.3f}]\")\n",
    "\n",
    "    # Making sure to save all these vmin,vmax,norm_us, etc in a json file for future use in task 3.\n",
    "    sample_dir = os.path.join(OUTPUT_ROOT, sample_name)\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "    # Quantize both\n",
    "    print(f\"\\n[2] Quantizing with {BINS} bins...\")\n",
    "    quant_mm, shifted_mm = quantize(norm_mm, bins=BINS)\n",
    "    quant_us, shifted_us = quantize(norm_us, bins=BINS)\n",
    "\n",
    "    params = {\n",
    "        \"vmin\": vmin.tolist(),\n",
    "        \"vmax\": vmax.tolist(),\n",
    "        \"centroid\": centroid.tolist(),\n",
    "        \"scale\": float(scale),\n",
    "        \"bins\": BINS,\n",
    "        \"shifted_mm\": shifted_mm,   # Min–Max normalization already in [0,1]\n",
    "        \"shifted_us\": shifted_us,    # Unit Sphere normalization mapped from [-1,1] → [0,1]\n",
    "        }             \n",
    "\n",
    "    params_path = os.path.join(sample_dir, \"normalization_params.json\")\n",
    "    \n",
    "    with open(params_path, \"w\") as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "        print(f\" Saved normalization parameters: {params_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\" Min–Max quantized: [{quant_mm.min()}, {quant_mm.max()}]\")\n",
    "    print(f\" Unit Sphere quantized: [{quant_us.min()}, {quant_us.max()}]\")\n",
    "    \n",
    "    # Create output directories\n",
    "    sample_dir = os.path.join(OUTPUT_ROOT, sample_name)\n",
    "    os.makedirs(os.path.join(sample_dir, \"normalized\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(sample_dir, \"quantized\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(sample_dir, \"plots\"), exist_ok=True)\n",
    "    \n",
    "    # Save all four meshes\n",
    "    print(\"\\n[3] Saving meshes...\")\n",
    "    trimesh.Trimesh(norm_mm, faces, process=False).export(\n",
    "        os.path.join(sample_dir, \"normalized\", \"minmax.ply\"))\n",
    "    trimesh.Trimesh(norm_us, faces, process=False).export(\n",
    "        os.path.join(sample_dir, \"normalized\", \"unitsphere.ply\"))\n",
    "    trimesh.Trimesh(quant_mm.astype(float), faces, process=False).export(\n",
    "        os.path.join(sample_dir, \"quantized\", \"minmax.ply\"))\n",
    "    trimesh.Trimesh(quant_us.astype(float), faces, process=False).export(\n",
    "        os.path.join(sample_dir, \"quantized\", \"unitsphere.ply\"))\n",
    "    \n",
    "    print(f\"  Saved normalized meshes: {sample_dir}/normalized/\")\n",
    "    print(f\"  Saved quantized meshes: {sample_dir}/quantized/\")\n",
    "\n",
    "    # Generate visualization\n",
    "    print(\"\\n[4] Creating visualization...\")\n",
    "    visualize_task2(norm_mm, norm_us, quant_mm, quant_us, faces, sample_name, sample_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Completed: {sample_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_meshes():\n",
    "    \"\"\"\n",
    "    Loop through all meshes in the dataset and process each for Task 2.\n",
    "    \"\"\"\n",
    "    # Find all .obj files in MESH_DIR\n",
    "    mesh_files = [f for f in os.listdir(MESH_DIR) if f.endswith('.obj')]\n",
    "    \n",
    "    if not mesh_files:\n",
    "        print(f\"No .obj files found in {MESH_DIR}/\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK 2: NORMALIZE & QUANTIZE MESHES\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Found {len(mesh_files)} mesh file(s)\")\n",
    "    print(f\"Output directory: {OUTPUT_ROOT}/\")\n",
    "    print(f\"Bins: {BINS}\")\n",
    "    \n",
    "    # Process each mesh\n",
    "    for mesh_file in mesh_files:\n",
    "        mesh_path = os.path.join(MESH_DIR, mesh_file)\n",
    "        sample_name = os.path.splitext(mesh_file)[0]\n",
    "\n",
    "        \n",
    "        try:\n",
    "            process_mesh_task2(mesh_path, sample_name)  \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ ERROR processing {mesh_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK 2 COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n Deliverables for each mesh:\")\n",
    "    print(f\"   normalized/minmax.ply\")\n",
    "    print(f\"   normalized/unitsphere.ply\")\n",
    "    print(f\"   quantized/minmax.ply\")\n",
    "    print(f\"   quantized/unitsphere.ply\")\n",
    "    print(f\"   plots/task2_results.png\")\n",
    "    print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14003605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_all_meshes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a567d9",
   "metadata": {},
   "source": [
    "Normalization is done to put the mesh in a common scale so that it can be better processed. The 2 such way to perform normalization on a mesh are as follows:-\n",
    "\n",
    "1. Min-max Normalization\n",
    "\n",
    "In Min-max we take the minimum and the maximum points in each axis x,y,z and create a box of 0 to 1 to squash these mesh coordinates in the box. Using min-max usually distorts the mesh structure as it needs to put all the coordinates into this box. Hence mesh structure is NOT preserved.\n",
    "\n",
    "2. Unit Sphere Normalization\n",
    "\n",
    "In Unit-Sphere we take a center point in the mesh and create a sphere. We then try to shrink the mesh structer uniformaly such that the farthest vertex is on the surface of the sphere radius=1. The structure is preserved and normalization is also achieved. Hence mesh structure is preserved and Normalization is achieved.\n",
    "\n",
    "-> Unit Sphere Normalization preserves the Mesh Structure and Normalize the mesh at the same time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d37e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_minmax(dequantized, vmin, vmax):\n",
    "    \"\"\"Denormalize Min-Max back to original scale.\"\"\"\n",
    "    return dequantized * (vmax - vmin) + vmin\n",
    "\n",
    "def denormalize_unitsphere(dequantized, centroid, scale):\n",
    "    \"\"\"Denormalize Unit Sphere back to original scale.\"\"\"\n",
    "    return dequantized * scale + centroid\n",
    "\n",
    "def dequantize(quantized, bins=1024, shifted=False):\n",
    "    \"\"\"Dequantize back to normalized coordinates.\"\"\"\n",
    "    # Convert to float and normalize to [0, 1]\n",
    "    dequantized = quantized.astype(float) / (bins - 1)\n",
    "    dequantized = np.clip(dequantized, 0.0, 1.0)  # Safety clamp\n",
    "    \n",
    "    if shifted:\n",
    "        # Shift back from [0, 1] to [-1, 1]\n",
    "        dequantized = dequantized * 2.0 - 1.0\n",
    "    \n",
    "    return dequantized\n",
    "\n",
    "def compute_errors(original, reconstructed):\n",
    "    \"\"\"Compute reconstruction error metrics.\"\"\"\n",
    "    mse = np.mean((original - reconstructed) ** 2)\n",
    "    mae = np.mean(np.abs(original - reconstructed))\n",
    "    mse_axis = np.mean((original - reconstructed) ** 2, axis=0)\n",
    "    mae_axis = np.mean(np.abs(original - reconstructed), axis=0)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae,\n",
    "        \"mse_axis\": mse_axis,\n",
    "        \"mae_axis\": mae_axis\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(original, recon_mm, recon_us, sample_name, sample_dir):\n",
    "    \"\"\"Visualize original vs reconstructed meshes (1×3 layout).\"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6), subplot_kw={'projection': '3d'})\n",
    "    fig.suptitle(f'Task 3: Reconstruction - {sample_name}', fontsize=15, fontweight='bold')\n",
    "    \n",
    "    def set_equal_axes(ax, data):\n",
    "        mins, maxs = data.min(axis=0), data.max(axis=0)\n",
    "        rng = maxs - mins\n",
    "        mid = (maxs + mins) / 2\n",
    "        max_range = rng.max() / 2\n",
    "        ax.set_xlim(mid[0]-max_range, mid[0]+max_range)\n",
    "        ax.set_ylim(mid[1]-max_range, mid[1]+max_range)\n",
    "        ax.set_zlim(mid[2]-max_range, mid[2]+max_range)\n",
    "    \n",
    "    plots = [\n",
    "        (axs[0], original, 'Original', 'gray'),\n",
    "        (axs[1], recon_mm, 'Min–Max Reconstructed', 'green'),\n",
    "        (axs[2], recon_us, 'Unit Sphere Reconstructed', 'blue')\n",
    "    ]\n",
    "    \n",
    "    for ax, data, title, color in plots:\n",
    "        ax.scatter(data[:,0], data[:,1], data[:,2], s=5, c=color, alpha=0.7)\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "        set_equal_axes(ax, data)\n",
    "        ax.grid(False)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plot_path = os.path.join(sample_dir, \"plots\", \"task3_reconstruction.png\")\n",
    "    plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"  Reconstruction visualization saved: {plot_path}\")\n",
    "\n",
    "def plot_error_per_axis(errors_mm, errors_us, sample_name, sample_dir):\n",
    "    \"\"\"Plot reconstruction error per axis (bar chart).\"\"\"\n",
    "    axes_labels = ['X', 'Y', 'Z']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle(f'Task 3: Reconstruction Error per Axis - {sample_name}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # MSE per axis\n",
    "    x = np.arange(len(axes_labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, errors_mm['mse_axis'], width, label='Min–Max', \n",
    "            alpha=0.8, color='green')\n",
    "    ax1.bar(x + width/2, errors_us['mse_axis'], width, label='Unit Sphere', \n",
    "            alpha=0.8, color='blue')\n",
    "    ax1.set_xlabel('Axis')\n",
    "    ax1.set_ylabel('MSE')\n",
    "    ax1.set_title('Mean Squared Error per Axis')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(axes_labels)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # MAE per axis\n",
    "    ax2.bar(x - width/2, errors_mm['mae_axis'], width, label='Min–Max', \n",
    "            alpha=0.8, color='green')\n",
    "    ax2.bar(x + width/2, errors_us['mae_axis'], width, label='Unit Sphere', \n",
    "            alpha=0.8, color='blue')\n",
    "    ax2.set_xlabel('Axis')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_title('Mean Absolute Error per Axis')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(axes_labels)\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plot_path = os.path.join(sample_dir, \"plots\", \"task3_error_plot.png\")\n",
    "    plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"  Error plot saved: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_task3(mesh_path, sample_name):\n",
    "    \"\"\"Process a single mesh for Task 3: Dequantize, Denormalize, Measure Error.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task 3 Processing: {sample_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    sample_dir = os.path.join(OUTPUT_ROOT, sample_name)\n",
    "    \n",
    "    # STEP 2 — Load required data\n",
    "    print(\"\\n[1] Loading data...\")\n",
    "    \n",
    "    # Load original mesh\n",
    "    mesh = trimesh.load(mesh_path, process=False)\n",
    "    original_vertices = np.array(mesh.vertices)\n",
    "    print(f\"  Loaded original: {len(original_vertices)} vertices\")\n",
    "    \n",
    "    # Load normalization parameters\n",
    "    params_path = os.path.join(sample_dir, \"normalization_params.json\")\n",
    "    if not os.path.exists(params_path):\n",
    "        print(f\"  Parameters not found: {params_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    vmin = np.array(params['vmin'])\n",
    "    vmax = np.array(params['vmax'])\n",
    "    centroid = np.array(params['centroid'])\n",
    "    scale = params['scale']\n",
    "    bins = params['bins']\n",
    "    shifted_mm = params['shifted_mm']\n",
    "    shifted_us = params['shifted_us']\n",
    "    print(f\"  Loaded normalization parameters\")\n",
    "    \n",
    "    # Load quantized meshes\n",
    "    quant_mm_mesh = trimesh.load(\n",
    "        os.path.join(sample_dir, \"quantized\", \"minmax.ply\"), process=False)\n",
    "    quant_us_mesh = trimesh.load(\n",
    "        os.path.join(sample_dir, \"quantized\", \"unitsphere.ply\"), process=False)\n",
    "    \n",
    "    quant_mm = np.array(quant_mm_mesh.vertices)\n",
    "    quant_us = np.array(quant_us_mesh.vertices)\n",
    "    print(f\"  Loaded quantized meshes\")\n",
    "    \n",
    "    # STEP 3 — Dequantize\n",
    "    print(\"\\n[2] Dequantizing...\")\n",
    "    dequant_mm = dequantize(quant_mm.astype(int), bins=bins, shifted=shifted_mm)\n",
    "    dequant_us = dequantize(quant_us.astype(int), bins=bins, shifted=shifted_us)\n",
    "    print(f\"  Dequantized both meshes\")\n",
    "    \n",
    "    # STEP 4 — Denormalize to original scale\n",
    "    print(\"\\n[3] Denormalizing to original scale...\")\n",
    "    recon_mm = denormalize_minmax(dequant_mm, vmin, vmax)\n",
    "    recon_us = denormalize_unitsphere(dequant_us, centroid, scale)\n",
    "    print(f\"  Reconstructed Min–Max mesh\")\n",
    "    print(f\"  Reconstructed Unit Sphere mesh\")\n",
    "    \n",
    "    # STEP 5 — Compute reconstruction errors\n",
    "    print(\"\\n[4] Computing reconstruction errors...\")\n",
    "    errors_mm = compute_errors(original_vertices, recon_mm)\n",
    "    errors_us = compute_errors(original_vertices, recon_us)\n",
    "    \n",
    "    print(f\"\\n  Min–Max Errors:\")\n",
    "    print(f\"    MSE: {errors_mm['mse']:.8f}\")\n",
    "    print(f\"    MAE: {errors_mm['mae']:.8f}\")\n",
    "    \n",
    "    print(f\"\\n  Unit Sphere Errors:\")\n",
    "    print(f\"    MSE: {errors_us['mse']:.8f}\")\n",
    "    print(f\"    MAE: {errors_us['mae']:.8f}\")\n",
    "    \n",
    "    # STEP 6 — Visualize reconstructed meshes\n",
    "    print(\"\\n[5] Creating reconstruction visualization...\")\n",
    "    visualize_reconstruction(original_vertices, recon_mm, recon_us, \n",
    "                           sample_name, sample_dir)\n",
    "    \n",
    "    # STEP 7 — Plot error per axis\n",
    "    print(\"\\n[6] Creating error plots...\")\n",
    "    plot_error_per_axis(errors_mm, errors_us, sample_name, sample_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task 3 Completed: {sample_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"mesh\": sample_name,\n",
    "        \"mm_mse\": errors_mm['mse'],\n",
    "        \"mm_mae\": errors_mm['mae'],\n",
    "        \"us_mse\": errors_us['mse'],\n",
    "        \"us_mae\": errors_us['mae'],\n",
    "        \"mm_mse_x\": errors_mm['mse_axis'][0],\n",
    "        \"mm_mse_y\": errors_mm['mse_axis'][1],\n",
    "        \"mm_mse_z\": errors_mm['mse_axis'][2],\n",
    "        \"us_mse_x\": errors_us['mse_axis'][0],\n",
    "        \"us_mse_y\": errors_us['mse_axis'][1],\n",
    "        \"us_mse_z\": errors_us['mse_axis'][2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2db1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_meshes_task3():\n",
    "    \"\"\"Process all meshes for Task 3.\"\"\"\n",
    "    mesh_files = [f for f in os.listdir(MESH_DIR) if f.endswith('.obj')]\n",
    "    \n",
    "    if not mesh_files:\n",
    "        print(f\"No .obj files found in {MESH_DIR}/\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK 3: DEQUANTIZE, DENORMALIZE, AND MEASURE ERROR\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Found {len(mesh_files)} mesh file(s)\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for mesh_file in mesh_files:\n",
    "        mesh_path = os.path.join(MESH_DIR, mesh_file)\n",
    "        sample_name = os.path.splitext(mesh_file)[0]\n",
    "        \n",
    "        try:\n",
    "            result = process_mesh_task3(mesh_path, sample_name)\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR processing {mesh_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # STEP 8 — Aggregate results\n",
    "    if all_results:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"AGGREGATE RESULTS - ALL MESHES\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Summary table\n",
    "        print(f\"\\n┌{'─'*30}┬{'─'*13}┬{'─'*13}┐\")\n",
    "        print(f\"│ {'Normalization Type':<28} │ {'Mean MSE':>11} │ {'Mean MAE':>11} │\")\n",
    "        print(f\"├{'─'*30}┼{'─'*13}┼{'─'*13}┤\")\n",
    "        \n",
    "        avg_mm_mse = np.mean([r['mm_mse'] for r in all_results])\n",
    "        avg_mm_mae = np.mean([r['mm_mae'] for r in all_results])\n",
    "        avg_us_mse = np.mean([r['us_mse'] for r in all_results])\n",
    "        avg_us_mae = np.mean([r['us_mae'] for r in all_results])\n",
    "        \n",
    "        print(f\"│ {'Min–Max':<28} │ {avg_mm_mse:>11.6f} │ {avg_mm_mae:>11.6f} │\")\n",
    "        print(f\"│ {'Unit Sphere':<28} │ {avg_us_mse:>11.6f} │ {avg_us_mae:>11.6f} │\")\n",
    "        print(f\"└{'─'*30}┴{'─'*13}┴{'─'*13}┘\\n\")\n",
    "        \n",
    "        # Save CSV\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(all_results)\n",
    "            csv_path = os.path.join(OUTPUT_ROOT, \"summary_task3_errors.csv\")\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\" Summary saved to: {csv_path}\")\n",
    "        except ImportError:\n",
    "            # Fallback without pandas\n",
    "            csv_path = os.path.join(OUTPUT_ROOT, \"summary_task3_errors.csv\")\n",
    "            with open(csv_path, 'w') as f:\n",
    "                f.write(\"mesh,mm_mse,mm_mae,us_mse,us_mae,mm_mse_x,mm_mse_y,mm_mse_z,us_mse_x,us_mse_y,us_mse_z\\n\")\n",
    "                for r in all_results:\n",
    "                    f.write(f\"{r['mesh']},{r['mm_mse']:.8f},{r['mm_mae']:.8f},\"\n",
    "                           f\"{r['us_mse']:.8f},{r['us_mae']:.8f},\"\n",
    "                           f\"{r['mm_mse_x']:.8f},{r['mm_mse_y']:.8f},{r['mm_mse_z']:.8f},\"\n",
    "                           f\"{r['us_mse_x']:.8f},{r['us_mse_y']:.8f},{r['us_mse_z']:.8f}\\n\")\n",
    "            print(f\" Summary saved to: {csv_path}\")\n",
    "        \n",
    "        # STEP 9 — Generate conclusion\n",
    "        conclusion = f\"\"\"\n",
    "{'='*70}\n",
    "TASK 3: RECONSTRUCTION ERROR ANALYSIS - CONCLUSION\n",
    "{'='*70}\n",
    "\n",
    "Summary Statistics:\n",
    "  Min-Max Normalization:\n",
    "    Mean MSE: {avg_mm_mse:.8f}\n",
    "    Mean MAE: {avg_mm_mae:.8f}\n",
    "\n",
    "  Unit Sphere Normalization:\n",
    "    Mean MSE: {avg_us_mse:.8f}\n",
    "    Mean MAE: {avg_us_mae:.8f}\n",
    "\n",
    "Analysis:\n",
    "\"\"\"\n",
    "        \n",
    "        if avg_mm_mse < avg_us_mse:\n",
    "            diff_pct = ((avg_us_mse - avg_mm_mse) / avg_us_mse) * 100\n",
    "            conclusion += f\"\"\"\n",
    "   Min-Max normalization consistently resulted in lower reconstruction \n",
    "   errors (MSE ≈ {avg_mm_mse:.8f}) compared to Unit Sphere normalization.\n",
    "    \n",
    "   Min-Max achieves {diff_pct:.1f}% lower MSE on average\n",
    "   This suggests axis-independent scaling better preserves geometric detail\n",
    "   through quantization for this dataset\n",
    "\"\"\"\n",
    "        else:\n",
    "            diff_pct = ((avg_mm_mse - avg_us_mse) / avg_mm_mse) * 100\n",
    "            conclusion += f\"\"\"\n",
    "   Unit Sphere normalization resulted in lower reconstruction errors \n",
    "   (MSE ≈ {avg_us_mse:.6f}) compared to Min-Max normalization.\n",
    "    \n",
    "  Unit Sphere achieves {diff_pct:.1f}% lower MSE on average\n",
    "  Uniform isotropic scaling better preserves geometric detail through\n",
    "  quantization for this dataset\n",
    "\"\"\"\n",
    "        \n",
    "        # Per-axis analysis\n",
    "        avg_mm_mse_x = np.mean([r['mm_mse_x'] for r in all_results])\n",
    "        avg_mm_mse_y = np.mean([r['mm_mse_y'] for r in all_results])\n",
    "        avg_mm_mse_z = np.mean([r['mm_mse_z'] for r in all_results])\n",
    "        \n",
    "        axis_errors = [('X', avg_mm_mse_x), ('Y', avg_mm_mse_y), ('Z', avg_mm_mse_z)]\n",
    "        max_axis = max(axis_errors, key=lambda x: x[1])\n",
    "        \n",
    "        conclusion += f\"\"\"\n",
    "Per-Axis Error Analysis:\n",
    "  • {max_axis[0]}-axis showed slightly higher error (MSE: {max_axis[1]:.8f})\n",
    "  • This may be due to scale compression during normalization or\n",
    "    quantization bin distribution along this axis\n",
    "\n",
    "Quantization Quality:\n",
    "  With {BINS} bins, both methods preserve geometric detail well\n",
    "  Average reconstruction error is very low (< 0.001 for most meshes)\n",
    " \n",
    "\n",
    "Recommendation:\n",
    "  For this dataset, {'Min-Max' if avg_mm_mse < avg_us_mse else 'Unit Sphere'} normalization is ideal\n",
    "  Error levels are acceptable for most 3D processing applications\n",
    "  \n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "        \n",
    "        conclusion_path = os.path.join(OUTPUT_ROOT, \"task3_conclusion.txt\")\n",
    "        with open(conclusion_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(conclusion)\n",
    "        \n",
    "        print(conclusion)\n",
    "        print(f\"Conclusion saved to: {conclusion_path}\\n\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TASK 3 COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_all_meshes_task3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transformed_meshes(original_mesh, num_transformations=NUM_TRANSFORMATIONS):\n",
    "    \"\"\"\n",
    "    Generate NUM_TRANSFORMATIONS-1 randomly transformed versions.\n",
    "    Returns: list of meshes [original, transformed1, transformed2, ...]\n",
    "    \"\"\"\n",
    "    print(f\"  [1] Generating {num_transformations} mesh versions...\")\n",
    "    \n",
    "    original_vertices = np.array(original_mesh.vertices)\n",
    "    faces = np.array(original_mesh.faces)\n",
    "    \n",
    "    transformed_meshes = [original_mesh]  # Include original\n",
    "    \n",
    "    for i in range(num_transformations - 1):\n",
    "        # Random rotation\n",
    "        random_rotation = Rotation.random().as_matrix()\n",
    "        \n",
    "        # Random translation (±0.1 range as specified)\n",
    "        random_translation = (np.random.rand(3) - 0.5) * 0.2\n",
    "        \n",
    "        # Apply transformation\n",
    "        new_vertices = (original_vertices @ random_rotation.T) + random_translation\n",
    "        \n",
    "        # Create new mesh\n",
    "        new_mesh = trimesh.Trimesh(new_vertices, faces, process=False)\n",
    "        transformed_meshes.append(new_mesh)\n",
    "    \n",
    "    print(f\"      ✓ Generated {num_transformations} versions (1 original + {num_transformations-1} transformed)\")\n",
    "    return transformed_meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd07f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertex_density(vertices, k=K_NEIGHBORS):\n",
    "    \"\"\"\n",
    "    Compute local vertex density using KDTree.\n",
    "    Returns: normalized density scores [0, 1] per vertex.\n",
    "    \"\"\"\n",
    "    print(f\"      Building KDTree...\")\n",
    "    tree = KDTree(vertices)\n",
    "    \n",
    "    print(f\"      Querying {k}-nearest neighbors...\")\n",
    "    # Query k+1 because point itself is included\n",
    "    distances, _ = tree.query(vertices, k=k+1)\n",
    "    \n",
    "    # Get average distance to k neighbors\n",
    "    avg_distances = distances[:, 1:].mean(axis=1)  # Exclude self\n",
    "    \n",
    "    # Density = inverse of average distance\n",
    "    safe_distances = np.maximum(avg_distances, 1e-9)\n",
    "    density_scores = 1.0 / safe_distances\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    min_score = density_scores.min()\n",
    "    max_score = density_scores.max()\n",
    "    range_score = max_score - min_score\n",
    "    \n",
    "    if range_score == 0:\n",
    "        normalized_scores = np.ones_like(density_scores) * 0.5\n",
    "    else:\n",
    "        normalized_scores = (density_scores - min_score) / range_score\n",
    "    \n",
    "    print(f\"      ✓ Density computed (range: [{normalized_scores.min():.3f}, {normalized_scores.max():.3f}])\")\n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_quantize(normalized_vertices, density_scores, base_bins=BINS, \n",
    "                     density_factor=ADAPTIVE_DENSITY_FACTOR):\n",
    "    \"\"\"\n",
    "    Adaptive quantization: variable bins based on density.\n",
    "    \"\"\"\n",
    "    # Shift to [0, 1] if needed\n",
    "    norm_01 = normalized_vertices.copy()\n",
    "    if norm_01.min() < -0.01:\n",
    "        norm_01 = (norm_01 + 1.0) / 2.0\n",
    "    \n",
    "    norm_01 = np.clip(norm_01, 0.0, 1.0)\n",
    "    \n",
    "    # Compute local bin counts per vertex\n",
    "    local_bins = base_bins * (1.0 + density_factor * density_scores)\n",
    "    \n",
    "    # Broadcast to (N, 3) for XYZ\n",
    "    local_bins_3d = np.repeat(local_bins[:, np.newaxis], 3, axis=1)\n",
    "    \n",
    "    # Quantize with variable bins per vertex\n",
    "    quantized = np.floor(norm_01 * (local_bins_3d - 1)).astype(np.int64)\n",
    "    \n",
    "    # Clip each vertex to its own max bin\n",
    "    for i in range(len(quantized)):\n",
    "        max_bin = int(local_bins_3d[i, 0] - 1)\n",
    "        quantized[i] = np.clip(quantized[i], 0, max_bin)\n",
    "    \n",
    "    return quantized, local_bins_3d\n",
    "\n",
    "def adaptive_dequantize(quantized_vertices, local_bins_3d):\n",
    "    \"\"\"\n",
    "    Dequantize using variable bin counts.\n",
    "    \"\"\"\n",
    "    # Dequantize to [0, 1]\n",
    "    dequant_01 = quantized_vertices.astype(float) / (local_bins_3d - 1)\n",
    "    \n",
    "    # Shift back to [-1, 1]\n",
    "    dequant_m1_1 = dequant_01 * 2.0 - 1.0\n",
    "    \n",
    "    return dequant_m1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_comparison(results_df, sample_name, sample_dir):\n",
    "    \"\"\"Plot Uniform vs Adaptive errors across transformations.\"\"\"\n",
    "    plot_path = os.path.join(sample_dir, \"plots\", \"error_comparison.png\")\n",
    "    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "    \n",
    "    n_groups = len(results_df)\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle(f'Task 4: Uniform vs Adaptive Quantization - {sample_name}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # MSE Plot\n",
    "    ax1.bar(index - bar_width/2, results_df['uniform_mse'], bar_width,\n",
    "            label='Uniform', color='royalblue', alpha=0.8)\n",
    "    ax1.bar(index + bar_width/2, results_df['adaptive_mse'], bar_width,\n",
    "            label='Adaptive', color='seagreen', alpha=0.8)\n",
    "    ax1.set_xlabel('Transformation')\n",
    "    ax1.set_ylabel('Mean Squared Error (MSE)')\n",
    "    ax1.set_title('MSE Comparison')\n",
    "    ax1.set_xticks(index)\n",
    "    ax1.set_xticklabels([f\"V{i}\" for i in range(n_groups)])\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # MAE Plot\n",
    "    ax2.bar(index - bar_width/2, results_df['uniform_mae'], bar_width,\n",
    "            label='Uniform', color='royalblue', alpha=0.8)\n",
    "    ax2.bar(index + bar_width/2, results_df['adaptive_mae'], bar_width,\n",
    "            label='Adaptive', color='seagreen', alpha=0.8)\n",
    "    ax2.set_xlabel('Transformation')\n",
    "    ax2.set_ylabel('Mean Absolute Error (MAE)')\n",
    "    ax2.set_title('MAE Comparison')\n",
    "    ax2.set_xticks(index)\n",
    "    ax2.set_xticklabels([f\"V{i}\" for i in range(n_groups)])\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"       Error comparison plot saved\")\n",
    "\n",
    "def plot_bin_distribution(density_scores, sample_name, sample_dir):\n",
    "    \"\"\"Plot distribution of adaptive bin sizes.\"\"\"\n",
    "    plot_path = os.path.join(sample_dir, \"plots\", \"bin_distribution.png\")\n",
    "    \n",
    "    local_bins = BINS * (1.0 + ADAPTIVE_DENSITY_FACTOR * density_scores)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(f'Task 4: Adaptive Bin Distribution - {sample_name}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Histogram\n",
    "    ax1.hist(local_bins, bins=50, color='seagreen', alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(BINS, color='red', linestyle='--', label=f'Base bins ({BINS})')\n",
    "    ax1.axvline(local_bins.mean(), color='blue', linestyle='--', label=f'Mean ({local_bins.mean():.0f})')\n",
    "    ax1.set_xlabel('Number of Bins')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Bin Counts')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Density vs Bins scatter\n",
    "    ax2.scatter(density_scores, local_bins, s=6, c=density_scores, \n",
    "               cmap='viridis', alpha=0.5)\n",
    "    ax2.set_xlabel('Normalized Density Score')\n",
    "    ax2.set_ylabel('Number of Bins')\n",
    "    ax2.set_title('Density vs Bin Count')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    cbar = plt.colorbar(ax2.collections[0], ax=ax2)\n",
    "    cbar.set_label('Density')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"       Bin distribution plot saved\")\n",
    "\n",
    "def plot_normalization_invariance(invariance_data, sample_name, sample_dir):\n",
    "    \"\"\"Plot normalization consistency across transformations.\"\"\"\n",
    "    plot_path = os.path.join(sample_dir, \"plots\", \"normalization_invariance.png\")\n",
    "    \n",
    "    versions = list(range(len(invariance_data['centroids_mm'])))\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Task 4: Normalization Invariance - {sample_name}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Min-Max Centroids\n",
    "    centroids_mm = np.array(invariance_data['centroids_mm'])\n",
    "    ax1.plot(versions, centroids_mm[:, 0], 'o-', label='X', color='red')\n",
    "    ax1.plot(versions, centroids_mm[:, 1], 's-', label='Y', color='green')\n",
    "    ax1.plot(versions, centroids_mm[:, 2], '^-', label='Z', color='blue')\n",
    "    ax1.set_xlabel('Transformation Version')\n",
    "    ax1.set_ylabel('Centroid Value')\n",
    "    ax1.set_title('Min-Max: Centroid Variation')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Unit Sphere Centroids\n",
    "    centroids_us = np.array(invariance_data['centroids_us'])\n",
    "    ax2.plot(versions, centroids_us[:, 0], 'o-', label='X', color='red')\n",
    "    ax2.plot(versions, centroids_us[:, 1], 's-', label='Y', color='green')\n",
    "    ax2.plot(versions, centroids_us[:, 2], '^-', label='Z', color='blue')\n",
    "    ax2.set_xlabel('Transformation Version')\n",
    "    ax2.set_ylabel('Centroid Value')\n",
    "    ax2.set_title('Unit Sphere: Centroid Variation')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # Min-Max Scale/Range\n",
    "    scales_mm = np.array(invariance_data['scales_mm'])\n",
    "    ax3.plot(versions, scales_mm, 'o-', color='purple', linewidth=2)\n",
    "    ax3.set_xlabel('Transformation Version')\n",
    "    ax3.set_ylabel('Scale Value')\n",
    "    ax3.set_title('Min-Max: Scale Variation')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # Unit Sphere Scale\n",
    "    scales_us = np.array(invariance_data['scales_us'])\n",
    "    ax4.plot(versions, scales_us, 'o-', color='orange', linewidth=2)\n",
    "    ax4.set_xlabel('Transformation Version')\n",
    "    ax4.set_ylabel('Scale Value')\n",
    "    ax4.set_title('Unit Sphere: Scale Variation')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"      ✓ Normalization invariance plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d036ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_task4(mesh_path, sample_name):\n",
    "    \"\"\"\n",
    "    Complete Task 4 pipeline for one mesh.\n",
    "    \n",
    "    Steps:\n",
    "    1. Generate transformed versions\n",
    "    2. Normalize each (both methods) and check invariance\n",
    "    3. Compute density\n",
    "    4. Quantize (uniform + adaptive)\n",
    "    5. Reconstruct\n",
    "    6. Compute errors\n",
    "    7. Visualize\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Task 4 Processing: {sample_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Setup directories\n",
    "    sample_dir = os.path.join(OUTPUT_ROOT_TASK4, sample_name)\n",
    "    subdirs = ['transformations', 'normalized', 'quantized_uniform', \n",
    "               'quantized_adaptive', 'reconstructions', 'plots']\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(sample_dir, subdir), exist_ok=True)\n",
    "    \n",
    "    # Load original mesh\n",
    "    original_mesh = trimesh.load(mesh_path, process=False)\n",
    "    original_vertices = np.array(original_mesh.vertices)\n",
    "    original_faces = np.array(original_mesh.faces)\n",
    "    \n",
    "    # STEP 1: Generate transformed meshes\n",
    "    transformed_meshes = generate_transformed_meshes(original_mesh, NUM_TRANSFORMATIONS)\n",
    "    \n",
    "    # Save transformations\n",
    "    for i, t_mesh in enumerate(transformed_meshes):\n",
    "        t_mesh.export(os.path.join(sample_dir, \"transformations\", f\"mesh_rot{i}.ply\"))\n",
    "    print(f\"        Saved {NUM_TRANSFORMATIONS} transformed meshes\")\n",
    "    \n",
    "    # Storage for results and invariance tracking\n",
    "    results = []\n",
    "    invariance_data = {\n",
    "        'centroids_mm': [],\n",
    "        'centroids_us': [],\n",
    "        'scales_mm': [],\n",
    "        'scales_us': []\n",
    "    }\n",
    "    \n",
    "    # Process each transformation\n",
    "    for version_id, t_mesh in enumerate(transformed_meshes):\n",
    "        print(f\"\\n  --- Version {version_id} ---\")\n",
    "        t_vertices = np.array(t_mesh.vertices)\n",
    "        \n",
    "        # STEP 2: Normalize (both methods)\n",
    "        print(f\"    [2] Normalizing...\")\n",
    "        \n",
    "        # Min-Max normalization\n",
    "        norm_mm, vmin_mm, vmax_mm = minmax_normalize(t_vertices)\n",
    "        scale_mm = np.max(vmax_mm - vmin_mm)  # Max range\n",
    "        centroid_mm = (vmax_mm + vmin_mm) / 2\n",
    "        \n",
    "        # Unit Sphere normalization\n",
    "        norm_us, centroid_us, scale_us = unit_sphere_normalize(t_vertices)\n",
    "        \n",
    "        # Track invariance metrics\n",
    "        invariance_data['centroids_mm'].append(centroid_mm)\n",
    "        invariance_data['centroids_us'].append(centroid_us)\n",
    "        invariance_data['scales_mm'].append(scale_mm)\n",
    "        invariance_data['scales_us'].append(scale_us)\n",
    "        \n",
    "        # Save normalized meshes\n",
    "        trimesh.Trimesh(norm_mm, original_faces, process=False).export(\n",
    "            os.path.join(sample_dir, \"normalized\", f\"v{version_id}_minmax.ply\"))\n",
    "        trimesh.Trimesh(norm_us, original_faces, process=False).export(\n",
    "            os.path.join(sample_dir, \"normalized\", f\"v{version_id}_unitsphere.ply\"))\n",
    "        \n",
    "        print(f\"         Min-Max: centroid={centroid_mm}, scale={scale_mm:.4f}\")\n",
    "        print(f\"         Unit Sphere: centroid={centroid_us}, scale={scale_us:.4f}\")\n",
    "        \n",
    "        # STEP 3: Compute vertex density (on Unit Sphere normalized)\n",
    "        print(f\"    [3] Computing vertex density...\")\n",
    "        density_scores = compute_vertex_density(norm_us, k=K_NEIGHBORS)\n",
    "        \n",
    "        # Save density data\n",
    "        np.save(os.path.join(sample_dir, \"normalized\", f\"v{version_id}_density.npy\"), \n",
    "                density_scores)\n",
    "        \n",
    "        # STEP 4: Quantization (Uniform baseline)\n",
    "        print(f\"    [4A] Uniform Quantization...\")\n",
    "        quant_uniform, _ = quantize(norm_us, bins=BINS)\n",
    "        \n",
    "        trimesh.Trimesh(quant_uniform.astype(float), original_faces, process=False).export(\n",
    "            os.path.join(sample_dir, \"quantized_uniform\", f\"v{version_id}.ply\"))\n",
    "        \n",
    "        # STEP 4: Quantization (Adaptive)\n",
    "        print(f\"    [4B] Adaptive Quantization...\")\n",
    "        quant_adaptive, local_bins = adaptive_quantize(norm_us, density_scores, \n",
    "                                                   base_bins=BINS,\n",
    "                                                   density_factor=ADAPTIVE_DENSITY_FACTOR)\n",
    "        \n",
    "        trimesh.Trimesh(quant_adaptive.astype(float), original_faces, process=False).export(\n",
    "        os.path.join(sample_dir, \"quantized_adaptive\", f\"v{version_id}.ply\"))\n",
    "\n",
    "        np.save(os.path.join(sample_dir, \"quantized_adaptive\", f\"v{version_id}_local_bins.npy\"), local_bins)\n",
    "        \n",
    "        print(f\"         Adaptive bins range: [{local_bins.min():.0f}, {local_bins.max():.0f}]\")\n",
    "        \n",
    "        # STEP 5: Reconstruction\n",
    "        print(f\"    [5] Reconstructing meshes...\")\n",
    "        \n",
    "        # Uniform reconstruction\n",
    "        dequant_uniform = dequantize(quant_uniform, bins=BINS, shifted=True)\n",
    "        recon_uniform = denormalize_unitsphere(dequant_uniform, centroid_us, scale_us)\n",
    "        \n",
    "        # Adaptive reconstruction\n",
    "        dequant_adaptive = adaptive_dequantize(quant_adaptive, local_bins)\n",
    "        recon_adaptive = denormalize_unitsphere(dequant_adaptive, centroid_us, scale_us)\n",
    "        \n",
    "        # Save reconstructions\n",
    "        trimesh.Trimesh(recon_uniform, original_faces, process=False).export(\n",
    "            os.path.join(sample_dir, \"reconstructions\", f\"v{version_id}_uniform.ply\"))\n",
    "        trimesh.Trimesh(recon_adaptive, original_faces, process=False).export(\n",
    "            os.path.join(sample_dir, \"reconstructions\", f\"v{version_id}_adaptive.ply\"))\n",
    "        \n",
    "        # STEP 6: Compute errors (compare to original, not transformed)\n",
    "        print(f\"    [6] Computing reconstruction errors...\")\n",
    "        errors_uniform = compute_errors(t_vertices, recon_uniform)\n",
    "        errors_adaptive = compute_errors(t_vertices, recon_adaptive)\n",
    "        \n",
    "        print(f\"        Uniform  - MSE: {errors_uniform['mse']:.2e}, MAE: {errors_uniform['mae']:.2e}\")\n",
    "        print(f\"        Adaptive - MSE: {errors_adaptive['mse']:.2e}, MAE: {errors_adaptive['mae']:.2e}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'version': version_id,\n",
    "            'uniform_mse': errors_uniform['mse'],\n",
    "            'uniform_mae': errors_uniform['mae'],\n",
    "            'adaptive_mse': errors_adaptive['mse'],\n",
    "            'adaptive_mae': errors_adaptive['mae'],\n",
    "            'mse_x_uniform': errors_uniform['mse_axis'][0],\n",
    "            'mse_y_uniform': errors_uniform['mse_axis'][1],\n",
    "            'mse_z_uniform': errors_uniform['mse_axis'][2],\n",
    "            'mse_x_adaptive': errors_adaptive['mse_axis'][0],\n",
    "            'mse_y_adaptive': errors_adaptive['mse_axis'][1],\n",
    "            'mse_z_adaptive': errors_adaptive['mse_axis'][2]\n",
    "        })\n",
    "    \n",
    "    # STEP 7: Generate visualizations\n",
    "    print(f\"\\n  [7] Generating visualizations...\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    plot_error_comparison(results_df, sample_name, sample_dir)\n",
    "    plot_bin_distribution(density_scores, sample_name, sample_dir)\n",
    "    plot_normalization_invariance(invariance_data, sample_name, sample_dir)\n",
    "    \n",
    "    # Save invariance log as JSON\n",
    "    invariance_log = {\n",
    "        'centroids_mm': [c.tolist() for c in invariance_data['centroids_mm']],\n",
    "        'centroids_us': [c.tolist() for c in invariance_data['centroids_us']],\n",
    "        'scales_mm': invariance_data['scales_mm'],\n",
    "        'scales_us': invariance_data['scales_us'],\n",
    "        'centroid_mm_std': np.std(invariance_data['centroids_mm'], axis=0).tolist(),\n",
    "        'centroid_us_std': np.std(invariance_data['centroids_us'], axis=0).tolist(),\n",
    "        'scale_mm_std': float(np.std(invariance_data['scales_mm'])),\n",
    "        'scale_us_std': float(np.std(invariance_data['scales_us']))\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(sample_dir, \"invariance_consistency_log.json\"), 'w') as f:\n",
    "        json.dump(invariance_log, f, indent=2)\n",
    "    print(f\"      Invariance consistency log saved\")\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    summary = {\n",
    "        'mesh': sample_name,\n",
    "        'mean_uniform_mse': results_df['uniform_mse'].mean(),\n",
    "        'std_uniform_mse': results_df['uniform_mse'].std(),\n",
    "        'mean_adaptive_mse': results_df['adaptive_mse'].mean(),\n",
    "        'std_adaptive_mse': results_df['adaptive_mse'].std(),\n",
    "        'mean_uniform_mae': results_df['uniform_mae'].mean(),\n",
    "        'mean_adaptive_mae': results_df['adaptive_mae'].mean(),\n",
    "        'scale_us_variance': invariance_log['scale_us_std']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  [8] Summary Statistics:\")\n",
    "    print(f\"      Uniform MSE:   {summary['mean_uniform_mse']:.2e} (±{summary['std_uniform_mse']:.2e})\")\n",
    "    print(f\"      Adaptive MSE:  {summary['mean_adaptive_mse']:.2e} (±{summary['std_adaptive_mse']:.2e})\")\n",
    "    print(f\"      Scale StdDev:  {summary['scale_us_variance']:.2e} (proves invariance)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✓ Task 4 Completed: {sample_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return summary, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6905ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_meshes_task4():\n",
    "    \"\"\"Process all meshes for Task 4.\"\"\"\n",
    "    mesh_files = [f for f in os.listdir(MESH_DIR) if f.endswith('.obj')]\n",
    "    \n",
    "    if not mesh_files:\n",
    "        print(f\"No .obj files found in {MESH_DIR}/\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK 4: INVARIANCE & ADAPTIVE QUANTIZATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Found {len(mesh_files)} mesh file(s)\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Transformations per mesh: {NUM_TRANSFORMATIONS}\")\n",
    "    print(f\"  - Base bins: {BINS}\")\n",
    "    print(f\"  - K-neighbors for density: {K_NEIGHBORS}\")\n",
    "    print(f\"  - Adaptive density factor: {ADAPTIVE_DENSITY_FACTOR}\")\n",
    "    print(f\"  - Output directory: {OUTPUT_ROOT_TASK4}/\")\n",
    "    \n",
    "    all_summaries = []\n",
    "    all_detailed_results = []\n",
    "    \n",
    "    for mesh_file in mesh_files:\n",
    "        mesh_path = os.path.join(MESH_DIR, mesh_file)\n",
    "        sample_name = os.path.splitext(mesh_file)[0]\n",
    "        \n",
    "        try:\n",
    "            summary, detailed_df = process_mesh_task4(mesh_path, sample_name)\n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "            # Add mesh name to detailed results\n",
    "            detailed_df['mesh'] = sample_name\n",
    "            all_detailed_results.append(detailed_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ ERROR processing {mesh_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Aggregate and save results\n",
    "    if all_summaries:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TASK 4: AGGREGATE RESULTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Save summary CSV\n",
    "        summary_df = pd.DataFrame(all_summaries)\n",
    "        summary_path = os.path.join(OUTPUT_ROOT_TASK4, \"task4_error_summary.csv\")\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"\\n  Summary saved: {summary_path}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        if all_detailed_results:\n",
    "            detailed_df_all = pd.concat(all_detailed_results, ignore_index=True)\n",
    "            detailed_path = os.path.join(OUTPUT_ROOT_TASK4, \"task4_detailed_results.csv\")\n",
    "            detailed_df_all.to_csv(detailed_path, index=False)\n",
    "            print(f\"  Detailed results saved: {detailed_path}\")\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        avg_uniform_mse = summary_df['mean_uniform_mse'].mean()\n",
    "        avg_adaptive_mse = summary_df['mean_adaptive_mse'].mean()\n",
    "        avg_scale_variance = summary_df['scale_us_variance'].mean()\n",
    "        avg_error_std = summary_df['std_uniform_mse'].mean()\n",
    "        \n",
    "        # Generate final conclusion\n",
    "        conclusion = f\"\"\"\n",
    "{'='*70}\n",
    "TASK 4: FINAL ANALYSIS & CONCLUSION\n",
    "{'='*70}\n",
    "\n",
    "Dataset: {len(all_summaries)} meshes\n",
    "Transformations per mesh: {NUM_TRANSFORMATIONS}\n",
    "Total analyses: {len(all_summaries) * NUM_TRANSFORMATIONS}\n",
    "\n",
    "{'='*70}\n",
    "PART 1: TRANSFORMATION INVARIANCE\n",
    "{'='*70}\n",
    "\n",
    "Objective: Verify that normalization is robust to rotation and translation.\n",
    "\n",
    "Method:\n",
    "  • Applied {NUM_TRANSFORMATIONS} random transformations (rotation + translation)\n",
    "  • Normalized each version using Unit Sphere method\n",
    "  • Measured consistency of scale and centroid across transformations\n",
    "\n",
    "Results:\n",
    "  • Average Scale Standard Deviation: {avg_scale_variance:.6e}\n",
    "  • Average MSE Standard Deviation: {avg_error_std:.6e}\n",
    "\n",
    "**Conclusion - Part 1:**\n",
    "The exceptionally low standard deviation ({avg_scale_variance:.6e}) confirms that\n",
    "the Unit Sphere normalization is highly invariant to transformations. The\n",
    "reconstruction error remains consistent regardless of the mesh's orientation\n",
    "or position, validating the normalization's robustness.\n",
    "\n",
    "{'='*70}\n",
    "PART 2: ADAPTIVE QUANTIZATION EFFECTIVENESS\n",
    "{'='*70}\n",
    "\n",
    "Objective: Compare uniform vs. adaptive quantization strategies.\n",
    "\n",
    "Configuration:\n",
    "  • Uniform: All vertices use {BINS} bins\n",
    "  • Adaptive: Bins range from {BINS} to {int(BINS * (1 + ADAPTIVE_DENSITY_FACTOR))}\n",
    "  • Allocation: Based on local vertex density (KNN with k={K_NEIGHBORS})\n",
    "\n",
    "Results:\n",
    "  • Average Uniform MSE:   {avg_uniform_mse:.8f}\n",
    "  • Average Adaptive MSE:  {avg_adaptive_mse:.8f}\n",
    "\"\"\"\n",
    "        \n",
    "        if avg_adaptive_mse < avg_uniform_mse:\n",
    "            improvement = ((avg_uniform_mse - avg_adaptive_mse) / avg_uniform_mse) * 100\n",
    "            conclusion += f\"\"\"\n",
    "**Conclusion - Part 2:**\n",
    "✓ Adaptive quantization was **SUCCESSFUL**.\n",
    "\n",
    "Adaptive quantization achieved {improvement:.2f}% lower MSE compared to uniform\n",
    "quantization. By allocating more quantization bins to geometrically dense\n",
    "regions, the adaptive method effectively:\n",
    "  • Preserves fine details in complex areas\n",
    "  • Reduces information loss during quantization\n",
    "  • Improves overall reconstruction fidelity\n",
    "\n",
    "This demonstrates that density-aware quantization is beneficial for meshes\n",
    "with non-uniform geometric complexity.\n",
    "\"\"\"\n",
    "        else:\n",
    "            difference = ((avg_adaptive_mse - avg_uniform_mse) / avg_uniform_mse) * 100\n",
    "            conclusion += f\"\"\"\n",
    "**Conclusion - Part 2:**\n",
    "✗ Adaptive quantization did NOT outperform uniform quantization.\n",
    "\n",
    "Adaptive quantization resulted in {abs(difference):.2f}% higher MSE compared\n",
    "to uniform quantization. Possible explanations:\n",
    "  • The meshes have relatively uniform geometric complexity\n",
    "  • The base bin count ({BINS}) is already sufficient for this dataset\n",
    "  • Density calculation (k={K_NEIGHBORS}) may need tuning\n",
    "  • Adaptive overhead doesn't justify benefits for these meshes\n",
    "\n",
    "For this dataset, uniform quantization with {BINS} bins appears optimal.\n",
    "\"\"\"\n",
    "        \n",
    "        conclusion += f\"\"\"\n",
    "{'='*70}\n",
    "RECOMMENDATIONS\n",
    "{'='*70}\n",
    "\n",
    "1. Normalization Choice:\n",
    "   ✓ Use Unit Sphere normalization for rotation/translation invariance\n",
    "   → Essential for applications requiring consistent representation\n",
    "      regardless of mesh orientation\n",
    "\n",
    "2. Quantization Strategy:\n",
    "\"\"\"\n",
    "        \n",
    "        if avg_adaptive_mse < avg_uniform_mse:\n",
    "            conclusion += f\"\"\"   ✓ Use adaptive quantization for meshes with:\n",
    "     - High geometric complexity variations\n",
    "     - Detailed features in specific regions\n",
    "     - Need for optimal detail preservation\n",
    "   → Provides {improvement:.1f}% better reconstruction accuracy\n",
    "\"\"\"\n",
    "        else:\n",
    "            conclusion += f\"\"\"   ✓ Use uniform quantization for:\n",
    "     - Meshes with uniform complexity distribution\n",
    "     - Applications prioritizing computational efficiency\n",
    "     - Datasets where {BINS} bins are sufficient\n",
    "   → Simpler, faster, and performs well on this dataset\n",
    "\"\"\"\n",
    "        \n",
    "        conclusion += f\"\"\"\n",
    "3. Parameter Tuning:\n",
    "   • Base bins: {BINS} (current)\n",
    "   • K-neighbors: {K_NEIGHBORS} (adjust based on mesh resolution)\n",
    "   • Density factor: {ADAPTIVE_DENSITY_FACTOR} (increase for more adaptation)\n",
    "\n",
    "{'='*70}\n",
    "END OF ANALYSIS\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "        \n",
    "        # Save conclusion\n",
    "        conclusion_path = os.path.join(OUTPUT_ROOT_TASK4, \"task4_conclusion.txt\")\n",
    "        with open(conclusion_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(conclusion)\n",
    "        \n",
    "        print(conclusion)\n",
    "        print(f\"\\n  ✓ Analysis saved: {conclusion_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TASK 4 COMPLETE\")\n",
    "    print(f\"{'='*70}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d808be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_all_meshes_task4()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
